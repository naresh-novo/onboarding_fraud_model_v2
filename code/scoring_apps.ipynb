{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    }
   ],
   "source": [
    "# path variables\n",
    "import sys\n",
    "project_path = '/Users/naresh/Downloads/ds_models/onboarding_fraud_model_v2/'\n",
    "sys.path.insert(0, project_path+'config')\n",
    "from config import SQLQuery\n",
    "\n",
    "# core libraries\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textstat\n",
    "import nltk\n",
    "import json\n",
    "import re\n",
    "import validators\n",
    "import requests\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "from model_evaluations import model_metrics, cross_validation\n",
    "from model_building import tune_hyperparameters\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, precision_score, recall_score, roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from fuzzywuzzy import fuzz\n",
    "from itertools import permutations\n",
    "from nltk.util import ngrams\n",
    "from nltk.collocations import BigramAssocMeasures, TrigramAssocMeasures, BigramCollocationFinder, TrigramCollocationFinder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom DS Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from stability_monitoring import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89792, 62)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw data - apps and alloy sources\n",
    "year = '2022_2023'\n",
    "file = 'apps_raw_dataset_2022.pkl'\n",
    "path = project_path + 'data/'\n",
    "df_raw = pd.read_pickle(path + file)\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting uppercase column names to lowercase column names\n",
    "cols_lower = []\n",
    "for col in df_raw.columns:\n",
    "    cols_lower.append(col.lower())\n",
    "    df_raw[col.lower()]=df_raw[col]\n",
    "\n",
    "df_raw = df_raw[cols_lower] # selecting only lowercase columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separating train and test data\n",
    "x_oot = df_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for raw features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_oot[['ein_ssn','has_international_business']] = x_oot[['ein_ssn','has_international_business']].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['person_fraud_score', 'person_kyc_score', 'sentilink_abuse_score', 'sentilink_first_party_synthetic_score'\n",
    "            ,'sentilink_third_party_synthetic_score', 'sentilink_id_theft_score', 'socure_sigma', 'socure_emailrisk', 'socure_phonerisk'\n",
    "            ,'socure_addressrisk','number_of_employees']\n",
    "num_features2 = ['business_address_zip', 'phone']\n",
    "bool_features = ['ein_ssn','has_international_business']\n",
    "cat_features = ['iovation_device_type', 'estimated_monthly_revenue', 'incoming_ach_payments', 'check_deposit_amount'\n",
    "               , 'incoming_wire_transfer', 'outgoing_ach_and_checks', 'outgoing_wire_transfers', 'line_type', 'industry_category_name']\n",
    "list_features = ['person_fraud_tags', 'person_kyc_tags', 'socure_reason_code', 'socure_kyc_field_validations'\n",
    "                ,'socure_kyc_reason_code', 'socure_emailrisk_reason_code', 'socure_phonerisk_reason_code'\n",
    "                ,'socure_addressrisk_reason_code', 'purpose_of_account', 'touch_point_emails', 'owner_list']\n",
    "high_cardinality_features = ['iovation_device_timezone', 'iovation_device_ip', 'iovation_device_ip_isp', 'iovation_device_ip_org' \n",
    "                            ,'iovation_device_ip_city', 'iovation_device_ip_region', 'carrier', 'email', 'email_domain', 'industry_name'\n",
    "                            , 'website', 'business_address_city', 'business_address_state', 'industry_category_from_pitch'\n",
    "                            , 'company_name']\n",
    "text_features = ['business_pitch']\n",
    "id_features = ['application_id', 'fraud_score', 'deposit_score']\n",
    "datetime_features = ['application_start_datetime', 'application_complete_datetime', 'application_resubmitted_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89792, 57)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_features = num_features + num_features2 + bool_features + cat_features + list_features + high_cardinality_features + text_features + id_features + datetime_features\n",
    "x_oot = x_oot[raw_features]\n",
    "x_oot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = x_oot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # convert all string features to lowercase\n",
    "# string_features = cat_features + list_features + high_cardinality_features + text_features\n",
    "\n",
    "# for col in string_features:\n",
    "#     print(col)\n",
    "#     x_train[col] = x_train[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts all type of nulls to one null format\n",
    "def convert_nulls_to_one_format(df:pd.DataFrame):\n",
    "    for col in df.columns:\n",
    "        idx = df.index[df[col].isnull()].tolist()\n",
    "        idx.extend(df.index[df[col].isna()].tolist())\n",
    "        idx.extend(df.index[df[col] == ''].tolist())\n",
    "        idx.extend(df.index[df[col] == '[]'].tolist())\n",
    "        idx = list(set(idx))\n",
    "        df.loc[idx, col] = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Saving the imputing values of each feature\n",
    "\n",
    "# # Numerical\n",
    "# df_impute_numerical = df[num_features].median()\n",
    "# df_impute_numerical = pd.DataFrame(df_impute_numerical, columns=['impute_value']).reset_index().rename(columns={'index':'feature'})\n",
    "\n",
    "\n",
    "# data_impute_list = []\n",
    "# for col in cat_features+high_cardinality_features+list_features+bool_features:\n",
    "#     if col not in ['email','touch_point_emails','owner_list','website','company_name']:\n",
    "#         mode_val = df[col].mode()\n",
    "#         if len(mode_val)==1:\n",
    "#             data_impute_list.append([col, mode_val[0]])\n",
    "#         else:\n",
    "#             data_impute_list.append([col, mode_val[0]])\n",
    "# # Categorical\n",
    "# df_impute_categorical = pd.DataFrame(data_impute_list, columns=['impute_value', 'feature'])\n",
    "\n",
    "# df_impute_custom = pd.DataFrame([['email','na'], ['touch_point_emails','na'], ['owner_list','na'], ['website','na'],\n",
    "#                                 ['company_name','na']], columns=['feature','impute_value'])\n",
    "# # Combining all imputes\n",
    "# df_impute = pd.concat([df_impute_numerical,df_impute_categorical, df_impute_custom], axis=0)\n",
    "# df_impute.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df_impute.to_pickle(project_path+'models/df_impute.pkl') # Save the impute values as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load impute df\n",
    "df_impute = pd.read_pickle(project_path+'models/df_impute_'+year+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute all the nulls\n",
    "def fill_null_values(df_impute:pd.DataFrame, data_df:pd.DataFrame,):\n",
    "    df_dict = dict(df_impute.values)\n",
    "    impute_cols = df_impute['feature'].to_list()\n",
    "    for col in data_df.columns.to_list():\n",
    "        if col in impute_cols:\n",
    "            data_df[col] = data_df[col].fillna(df_dict[col])\n",
    "            impute_cols.remove(col)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert all nulls to one format\n",
    "df = convert_nulls_to_one_format(df=df)\n",
    "# Data imputing\n",
    "df = fill_null_values(df_impute, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean Features Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ein_ssn'] = df['ein_ssn']*1\n",
    "df['ein_ssn'] = df['ein_ssn'].astype(int)\n",
    "df['has_international_business'] = df['has_international_business']*1\n",
    "df['has_international_business'] = df['has_international_business'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime Fetaures Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling nulls with the start date because the difference between start date and complete date remains less than 24 hours\n",
    "df['application_complete_datetime'] = np.where(df['application_complete_datetime'].isnull(), df['application_start_datetime'], \n",
    "                                               df['application_complete_datetime'])\n",
    "df['application_start_datetime'] = pd.to_datetime(df['application_start_datetime'])\n",
    "df['application_complete_datetime'] = pd.to_datetime(df['application_complete_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert day of month to the week of month\n",
    "def weekofmonth(val):\n",
    "    if val<=7:\n",
    "        return 1\n",
    "    elif val<=14:\n",
    "        return 2\n",
    "    elif val<=21:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "# Correcting the features values to standard format\n",
    "replace_weekdaytonumber = {'sunday':0, 'monday':1, 'tuesday':2 , 'wednesday':3, 'thursday':4, 'friday':5, 'saturday':6}\n",
    "\n",
    "# App start date features\n",
    "# df['app_start_monthofyear'] = df['application_start_datetime'].dt.month\n",
    "df['app_start_dateofmonth'] = df['application_start_datetime'].dt.day\n",
    "df['app_start_weekofmonth'] = df['app_start_dateofmonth'].apply(weekofmonth)\n",
    "df['app_start_dayofweek'] = df['application_start_datetime'].dt.day_name()\n",
    "# df['app_start_dayofweek'] = df['app_start_dayofweek'].astype('string').apply(weekdaytonumber)\n",
    "df['app_start_dayofweek'] = df['app_start_dayofweek'].replace(replace_weekdaytonumber)\n",
    "df['app_start_hourofday'] = df['application_start_datetime'].dt.hour\n",
    "\n",
    "# App complete date features\n",
    "# df['app_complete_monthofyear'] = df['application_complete_datetime'].dt.month\n",
    "df['app_complete_dateofmonth'] = df['application_complete_datetime'].dt.day\n",
    "df['app_complete_weekofmonth'] = df['app_complete_dateofmonth'].apply(weekofmonth)\n",
    "df['app_complete_dayofweek'] = df['application_complete_datetime'].dt.day_name()\n",
    "# df['app_complete_dayofweek'] = df['app_complete_dayofweek'].astype('string').apply(weekdaytonumber)\n",
    "df['app_complete_dayofweek'] = df['app_complete_dayofweek'].replace(replace_weekdaytonumber)\n",
    "df['app_complete_hourofday'] = df['application_complete_datetime'].dt.hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = 'iovation_device_type'\n",
    "df[col] = df[col].str.lower()\n",
    "df[col] = np.where(df[col].isin(['iphone', 'mac','ipad', 'ipod']), 'apple', df[col])\n",
    "df[col] = np.where(df[col].isin(['chromeos', 'linux', 'handheld_other']), 'other', df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_features = ['estimated_monthly_revenue','incoming_ach_payments','check_deposit_amount','incoming_wire_transfer',\n",
    "                      'outgoing_ach_and_checks','outgoing_wire_transfers']\n",
    "\n",
    "# Correcting the features values to standard format\n",
    "estimated_features_replace_values1 = {'$0':'0k', \n",
    "                                      '$0 - $1k':'1k', '<$1k':'1k', '$1 - $1k':'1k', \n",
    "                                      '$1k - $5k':'1k_plus', '$1k +':'1k_plus', \n",
    "                                      '$5k - $20k':'5k_plus', '$5k +':'5k_plus', '$20k - $50k':'5k_plus', \n",
    "                                      '$50k +':'50k_plus' }\n",
    "for col in estimated_features:\n",
    "    df[col] = df[col].str.lower()\n",
    "df[estimated_features] = df[estimated_features].replace(estimated_features_replace_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_features_interaction = []\n",
    "for value in ['0k','1k','1k_plus','5k_plus','50k_plus']:\n",
    "    estimated_features_interaction.append('estimated_features_'+value+'_count')\n",
    "    df['estimated_features_'+value+'_count'] = df[estimated_features][df[estimated_features]==value].count(axis=1)\n",
    "\n",
    "df['estimated_features_same_value_flag'] = df[df[estimated_features_interaction]==6].any(axis=1)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_features_outgoing = ['outgoing_ach_and_checks', 'outgoing_wire_transfers']\n",
    "estimated_features_incoming = list(set(estimated_features)-set(estimated_features_outgoing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace the ordinal values into numerical ordinal values\n",
    "estimated_features_replace_values2 = {'0k':0, '1k':500, '1k_plus':1000, '5k_plus':5000, '50k_plus':50000}\n",
    "df[estimated_features] = df[estimated_features].replace(estimated_features_replace_values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Deriving estimated interaction features\n",
    "df['total_incoming_vol'] = df[estimated_features_incoming].sum(axis=1)\n",
    "df['total_outgoing_vol'] = df[estimated_features_outgoing].sum(axis=1)\n",
    "df['total_txns_vol'] = df['total_incoming_vol'] + df['total_outgoing_vol']\n",
    "df['cashflow'] = df['total_incoming_vol'] - df['total_outgoing_vol']\n",
    "\n",
    "df['ach_mrdc_incoming_vol'] = df[['incoming_ach_payments','check_deposit_amount']].sum(axis=1) \n",
    "df['ach_mrdc_outgoing_vol'] = df['outgoing_ach_and_checks']\n",
    "df['ach_mrdc_total_txns_vol'] = df['ach_mrdc_incoming_vol'] + df['ach_mrdc_outgoing_vol']\n",
    "df['ach_mrdc_cashflow'] = df['ach_mrdc_incoming_vol'] - df['ach_mrdc_outgoing_vol']\n",
    "\n",
    "df['wire_incoming_vol'] = df['incoming_wire_transfer']\n",
    "df['wire_outgoing_vol'] = df['outgoing_wire_transfers']\n",
    "df['wire_total_txns_vol'] = df['wire_incoming_vol'] + df['wire_outgoing_vol']\n",
    "df['wire_cashflow'] = df['wire_incoming_vol'] - df['wire_outgoing_vol']\n",
    "\n",
    "\n",
    "# Ratio's\n",
    "replace_dict = {np.nan:0, np.inf:0, -np.inf:0}\n",
    "df['outgoing_to_incoming_ratio'] = df['total_outgoing_vol'].div(df['total_incoming_vol']).replace(replace_dict)\n",
    "df['cashflow_to_total_txns_vol_ratio'] = df['cashflow'].div(df['total_txns_vol']).replace(replace_dict)\n",
    "df['wire_outgoing_to_incoming_ratio'] = df['outgoing_wire_transfers'].div(df['total_incoming_vol']).replace(replace_dict)\n",
    "df['ach_mrdc_outgoing_to_incoming_ratio'] = df['outgoing_ach_and_checks'].div(df[['incoming_ach_payments','check_deposit_amount'\n",
    "                                                                                 ]].sum(axis=1)).replace(replace_dict)\n",
    "df['incoming_ach_to_revenue_ratio'] = df['incoming_ach_payments'].div(df['estimated_monthly_revenue']).replace(replace_dict)\n",
    "df['incoming_ach_mrdc_to_revenue_ratio'] = df[['incoming_ach_payments','check_deposit_amount']].sum(axis=1).div(df['estimated_monthly_revenue']).replace(replace_dict)\n",
    "\n",
    "df['ach_mrdc_total_txns_vol_to_total_txns_vol_ratio'] = (df['ach_mrdc_incoming_vol']+ df['ach_mrdc_outgoing_vol'])/df['total_txns_vol']\n",
    "df['ach_mrdc_incoming_to_total_incoming_vol_ratio'] = df['ach_mrdc_incoming_vol']/df['total_incoming_vol']\n",
    "df['ach_mrdc_outgoing_to_total_outgoing_vol_ratio'] = df['ach_mrdc_outgoing_vol']/df['total_outgoing_vol']\n",
    "\n",
    "df['wire_total_txns_vol_to_total_txns_vol_ratio'] = (df['wire_incoming_vol']+ df['wire_outgoing_vol'])/df['total_txns_vol']\n",
    "df['wire_incoming_to_total_incoming_vol_ratio'] = df['wire_incoming_vol']/df['total_incoming_vol']\n",
    "df['wire_outgoing_to_total_outgoing_vol_ratio'] = df['wire_outgoing_vol']/df['total_outgoing_vol']\n",
    "\n",
    "# Ratios of number of employees\n",
    "df['total_incoming_vol_to_employees_ratio'] = df['total_incoming_vol'].div(df['number_of_employees']).replace(replace_dict)\n",
    "df['total_outgoing_vol_to_employees_ratio'] = df['total_outgoing_vol'].div(df['number_of_employees']).replace(replace_dict)\n",
    "df['total_txns_vol_to_employees_ratio'] = df['total_txns_vol'].div(df['number_of_employees']).replace(replace_dict)\n",
    "df['cashflow_to_employees_ratio'] = df['cashflow'].div(df['number_of_employees']).replace(replace_dict)\n",
    "\n",
    "df['wire_incoming_vol_to_employees_ratio'] = df['wire_incoming_vol'].div(df['number_of_employees']).replace(replace_dict)\n",
    "df['wire_outgoing_vol_to_employees_ratio'] = df['wire_outgoing_vol'].div(df['number_of_employees']).replace(replace_dict)\n",
    "df['wire_total_txns_vol_to_employees_ratio'] = df['wire_total_txns_vol'].div(df['number_of_employees']).replace(replace_dict)\n",
    "df['wire_cashflow_to_employees_ratio'] = df['wire_cashflow'].div(df['number_of_employees']).replace(replace_dict)\n",
    "\n",
    "df['ach_mrdc_incoming_vol_to_employees_ratio'] = df['ach_mrdc_incoming_vol'].div(df['number_of_employees']).replace(replace_dict)\n",
    "df['ach_mrdc_outgoing_vol_to_employees_ratio'] = df['ach_mrdc_outgoing_vol'].div(df['number_of_employees']).replace(replace_dict)\n",
    "df['ach_mrdc_total_txns_vol_to_employees_ratio'] = df['ach_mrdc_total_txns_vol'].div(df['number_of_employees']).replace(replace_dict)\n",
    "df['ach_mrdc_cashflow_to_employees_ratio'] = df['ach_mrdc_cashflow'].div(df['number_of_employees']).replace(replace_dict)\n",
    "\n",
    "df['estimated_monthly_revenue_to_employees_ratio'] = df['estimated_monthly_revenue'].div(df['number_of_employees']).replace(replace_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tot = 0\n",
    "# tot_fraud = 0\n",
    "# for col in estimated_features_interaction:\n",
    "#     print(col, ':',df[df[col]==6].shape[0], df[df[col]==6].target.mean())\n",
    "#     tot = tot+df[df[col]==6].shape[0]\n",
    "#     tot_fraud = tot_fraud + df[df[col]==6].target.sum()\n",
    "# tot, tot_fraud, tot_fraud/tot, tot/df.shape[0], df.target.sum(), (df.target.sum()-tot_fraud)/(df.shape[0]-tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grouping the features values to the observed variance\n",
    "def estimated_feature_engg(value):\n",
    "    if value == None:\n",
    "        return None\n",
    "    value = value.lower()\n",
    "    if value in ['$0', '$0 - $1k', '<$1k']:\n",
    "        # return 'upto_1k'\n",
    "        return 0\n",
    "    elif value in ['$1k +', '$5k +', '$50k +']:\n",
    "        # return '1k_plus'\n",
    "        return 1\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "# Grouping the features values to the observed variance\n",
    "def estimated_feature_engg(value):\n",
    "    if value == None:\n",
    "        return None\n",
    "    if value<=500:\n",
    "        return 0\n",
    "    elif value>=1000:\n",
    "        return 1\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "for col in estimated_features:\n",
    "    df[col] = df[col].apply(estimated_feature_engg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'industry_category_name'\n",
    "df[col] = df[col].str.lower()\n",
    "df[col] = np.where(df[col].isin(['utilities','mining', 'agriculture, forestry, fishing and hunting', 'wholesale trade', \n",
    "                                 'accommodation and food services', 'administrative and support and waste management and remediation services',\n",
    "                                 'construction', 'finance and insurance', 'mining', 'other services', 'health care and social assistance',\n",
    "                                'manufacturing', 'public administration']), 'cat1', 'cat2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Cardinality Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'iovation_device_timezone'\n",
    "df[col] = df[col].astype('float64')\n",
    "df[col] = np.where(df[col].isin([300,360,480]), df[col], 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'email_domain'\n",
    "l1 = ['gmail.com']\n",
    "l2 = ['yahoo.com','outlook.com','icloud.com','hotmail.com','aol.com','protonmail.com']\n",
    "df[col] = np.where(df[col].isin(l1), 'l1', np.where(df[col].isin(l2), 'l2', 'other'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'iovation_device_ip_isp'\n",
    "df[col] = df[col].str.lower()\n",
    "df[col] = np.where(df[col].str.contains('t-mobile*'),'tmobile',df[col])\n",
    "df[col] = np.where(df[col].str.contains('verizon'),'verizon',df[col])\n",
    "df[col] = np.where(df[col].str.contains('at&t'),'att',df[col])\n",
    "df[col] = np.where(df[col].str.match('att'),'att',df[col])\n",
    "df[col] = np.where(df[col].str.match('charter [b|c]'),'charter',df[col])\n",
    "df[col] = np.where(df[col].str.match('comcast'),'comcast',df[col])\n",
    "df[col] = np.where(df[col].isin(['tmobile','att','charter','verizon','comcast']),df[col],'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'iovation_device_ip_org'\n",
    "df[col] = df[col].str.lower()\n",
    "df[col] = np.where(df[col].str.contains('t-mobile*'),'tmobile',df[col])\n",
    "df[col] = np.where(df[col].str.contains('verizon'),'verizon',df[col])\n",
    "df[col] = np.where(df[col].str.contains('at&t'),'att',df[col])\n",
    "df[col] = np.where(df[col].str.match('att'),'att',df[col])\n",
    "df[col] = np.where(df[col].str.match('charter [b|c]'),'charter',df[col])\n",
    "df[col] = np.where(df[col].str.match('comcast'),'comcast',df[col])\n",
    "df[col] = np.where(df[col].isin(['tmobile','att','charter','verizon','comcast']),df[col],'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'carrier'\n",
    "df[col] = df[col].str.lower()\n",
    "df[col] = np.where(df[col].str.contains('t-mobile*'),'tmobile',df[col])\n",
    "df[col] = np.where(df[col].str.contains('verizon'),'verizon',df[col])\n",
    "df[col] = np.where(df[col].str.contains('at&t'),'att',df[col])\n",
    "df[col] = np.where(df[col].str.match('att'),'att',df[col])\n",
    "df[col] = np.where(df[col].isin(['tmobile','att','verizon']),df[col],'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iovation_device_timezone',\n",
       " 'iovation_device_ip',\n",
       " 'iovation_device_ip_isp',\n",
       " 'iovation_device_ip_org',\n",
       " 'iovation_device_ip_city',\n",
       " 'iovation_device_ip_region',\n",
       " 'carrier',\n",
       " 'email',\n",
       " 'email_domain',\n",
       " 'industry_name',\n",
       " 'website',\n",
       " 'business_address_city',\n",
       " 'business_address_state',\n",
       " 'industry_category_from_pitch',\n",
       " 'company_name']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_cardinality_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_address_class(val:str):\n",
    "    if val==None or val=='':\n",
    "        return None\n",
    "    else:\n",
    "        if len(val)<=15:\n",
    "            val_split = val.split('.')\n",
    "            if 0<=int(val_split[0])<=127:\n",
    "                class_val = 'a'\n",
    "            elif 128<=int(val_split[0])<=191:\n",
    "                class_val = 'b'\n",
    "            elif 192<=int(val_split[0])<=223:\n",
    "                class_val = 'c'\n",
    "            elif 224<=int(val_split[0])<=239:\n",
    "                class_val = 'd'\n",
    "            elif 240<=int(val_split[0])<=255:\n",
    "                class_val = 'e'\n",
    "            else:\n",
    "                class_val = 'na'\n",
    "            return class_val\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "col = 'iovation_device_ip'\n",
    "df['ip_class'] = df[col].apply(ip_address_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "business_pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_readability_features(text):\n",
    "    if text==None or text=='':\n",
    "        return_features = {'flesch_reading_ease': 0, 'smog_index': 0, 'flesch_kincaid_grade': 0, 'coleman_liau_index': 0, \n",
    "                           'automated_readability_index': 0, 'dale_chall_readability_score': 0, 'difficult_words': 0,\n",
    "                           'linsear_write_formula': 0, 'gunning_fog': 0, 'text_standard': '-1th and 0th grade'}\n",
    "        return_features = tuple(return_val.values())\n",
    "    \n",
    "    else:\n",
    "        return_features = {\n",
    "            'flesch_reading_ease': textstat.flesch_reading_ease(text),\n",
    "            'smog_index': textstat.smog_index(text),\n",
    "            'flesch_kincaid_grade': textstat.flesch_kincaid_grade(text),\n",
    "            'coleman_liau_index': textstat.coleman_liau_index(text),\n",
    "            'automated_readability_index': textstat.automated_readability_index(text),\n",
    "            'dale_chall_readability_score': textstat.dale_chall_readability_score(text),\n",
    "            'difficult_words': textstat.difficult_words(text),\n",
    "            'linsear_write_formula': textstat.linsear_write_formula(text),\n",
    "            'gunning_fog': textstat.gunning_fog(text),\n",
    "            'text_standard': textstat.text_standard(text)\n",
    "        }\n",
    "        return_features = tuple(return_features.values())\n",
    "        \n",
    "    return return_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89792, 109)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = 'business_pitch'\n",
    "readability_columns = ['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index', 'automated_readability_index',\n",
    "                       'dale_chall_readability_score', 'difficult_words', 'linsear_write_formula', 'gunning_fog', 'text_standard']\n",
    "df[readability_columns] = pd.DataFrame(df[col].apply(extract_readability_features).tolist(),index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# engg text_standard feature\n",
    "# text_standard_vals = list(set(df['text_standard'].values))\n",
    "# text_standard_vals.sort()\n",
    "# text_standard_vals_replace = [0, -1, 1, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3, 31, 32, 326, \n",
    "#                               33, 34, 35, 36, 37, 39, 4, 41, 43, 5, 54, 6, 68, 7, 8, 9, 10]\n",
    "# text_standard_vals_replace = [x + 1 for x in text_standard_vals_replace]\n",
    "# df['text_standard_ordinals'] = df['text_standard'].replace(text_standard_vals, text_standard_vals_replace)\n",
    "\n",
    "# text_standard feature grouping\n",
    "df['text_standard_levels'] = np.where(df['text_standard'].isin(['6th and 7th grade','7th and 8th grade','8th and 9th grade', '9th and 10th grade']\n",
    "                                               ), 'l2', (np.where(df['text_standard'].isin(['-1th and 0th grade','0th and 1st grade','1st and 2nd grade','2nd and 3rd grade','3rd and 4th grade',\n",
    "                                                                                            '4th and 5th grade','5th and 6th grade']), 'l1', 'l3'))\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linguistic features\n",
    "def extract_linguistic_features(text):\n",
    "   \n",
    "    if text==None or text=='':\n",
    "        return (0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\n",
    "    else:\n",
    "        # print(text,'\\n\\n')\n",
    "        words = nltk.word_tokenize(text)\n",
    "        total_tokens = len(words)\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        syllable_count = sum(textstat.syllable_count(word) for word in words)\n",
    "        polysyllable_count = len([word for word in words if textstat.syllable_count(word) >= 3])\n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "        pos_counts = Counter(tag for word, tag in pos_tags)\n",
    "\n",
    "        # POS Tag Ratios\n",
    "        pos_ratios = {tag: count / total_tokens for tag, count in pos_counts.items()}\n",
    "        pos_ratios = dict(zip(['pos_ratio_'+key for key in pos_ratios.keys()], pos_ratios.values())) # adding prefix to the keys\n",
    "\n",
    "        # # POS Tag Sequences\n",
    "        # pos_sequence = ' '.join(tag for word, tag in pos_tags)\n",
    "        # words_pos = nltk.word_tokenize(pos_sequence)\n",
    "\n",
    "        # # POS Tag N-grams\n",
    "        # bigrams = list(ngrams([tag for word, tag in pos_tags], 2))\n",
    "        # trigrams = list(ngrams([tag for word, tag in pos_tags], 3))\n",
    "        # # Count bigrams and trigrams\n",
    "        # bigram_counts = Counter(bigrams)\n",
    "        # trigram_counts = Counter(trigrams)\n",
    "\n",
    "        # # Find important bigrams using mutual information\n",
    "        # bigram_measures = BigramAssocMeasures()\n",
    "        # bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "        # important_bigrams = bigram_finder.nbest(bigram_measures.pmi, 3)\n",
    "        \n",
    "        # # Find important trigrams using mutual information\n",
    "        # trigram_measures = TrigramAssocMeasures()\n",
    "        # trigram_finder = TrigramCollocationFinder.from_words(words)\n",
    "        # important_trigrams = trigram_finder.nbest(trigram_measures.pmi, 3)\n",
    "\n",
    "        # POS Diversity\n",
    "        pos_diversity = len(pos_counts) / total_tokens\n",
    "\n",
    "        # Custom POS Tag Features (e.g., Noun to Verb Ratio)\n",
    "        noun_count = sum(count for tag, count in pos_counts.items() if tag.startswith('NN'))\n",
    "        verb_count = sum(count for tag, count in pos_counts.items() if tag.startswith('VB'))\n",
    "        noun_verb_ratio = noun_count / (verb_count + 1)\n",
    "        \n",
    "        blob = TextBlob(text)\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        vader_sentiment = analyzer.polarity_scores(text)\n",
    "\n",
    "        pos_counts = dict(zip(['pos_count_'+key for key in pos_counts.keys()], pos_counts.values())) # adding prefix to the keys\n",
    "        linguistic_features = {\n",
    "            'total_word_count': total_tokens,\n",
    "            'sentence_count': len(sentences),\n",
    "            'average_sentence_length': total_tokens / len(sentences),\n",
    "            'average_word_length': sum(len(word) for word in words) / total_tokens,\n",
    "            'syllable_count': syllable_count,\n",
    "            'polysyllable_count': polysyllable_count,\n",
    "            'unique_words_count': len(set(words)),\n",
    "            'type_token_ratio': len(set(words)) / total_tokens,\n",
    "            'lexical_density': sum(1 for tag in pos_tags if tag[1] in ['NN', 'VB', 'JJ', 'RB']) / total_tokens,\n",
    "            'pos_counts': pos_counts,\n",
    "            'textblob_polarity': blob.sentiment.polarity, \n",
    "            'textblob_subjectivity': blob.sentiment.subjectivity,\n",
    "            'vader_neg': vader_sentiment['neg'],\n",
    "            'vader_neu': vader_sentiment['neu'],\n",
    "            'vader_pos': vader_sentiment['pos'],\n",
    "            'vader_compound': vader_sentiment['compound'],\n",
    "            'pos_diversity': pos_diversity,\n",
    "            'noun_to_verb_ratio': noun_verb_ratio,\n",
    "            'pos_ratios':pos_ratios\n",
    "\n",
    "        }\n",
    "        linguistic_features = tuple(linguistic_features.values())\n",
    "        \n",
    "        return linguistic_features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = 'business_pitch'\n",
    "linguistic_columns = ['total_word_count','sentence_count','average_sentence_length','average_word_length','syllable_count',\n",
    "                      'polysyllable_count','unique_words_count','type_token_ratio','lexical_density','pos_counts','textblob_polarity',\n",
    "                      'textblob_subjectivity','vader_neg','vader_neu','vader_pos','vader_compound','pos_diversity','noun_to_verb_ratio',\n",
    "                      'pos_ratios']\n",
    "df[linguistic_columns] = pd.DataFrame(df[col].apply(extract_linguistic_features).tolist(),index=df.index)\n",
    "\n",
    "# exploding POS tags count features and creating important columns\n",
    "pos_tags_count_features = ['pos_count_NN','pos_count_NNS','pos_count_NNP','pos_count_NNPS','pos_count_VB','pos_count_VBP','pos_count_VBG', \n",
    "                           'pos_count_VBN','pos_count_VBZ','pos_count_VBD','pos_count_CC','pos_count_PRP','pos_count_JJ','pos_count_TO',\n",
    "                           'pos_count_IN','pos_count_RB']\n",
    "df = pd.concat([df,pd.json_normalize(df['pos_counts'])[pos_tags_count_features].fillna(0)], axis=1)\n",
    "\n",
    "# exploding POS tags ratio features and creating important columns\n",
    "pos_tags_ratio_features = ['pos_ratio_NN','pos_ratio_NNS','pos_ratio_NNP','pos_ratio_NNPS','pos_ratio_VB','pos_ratio_VBP','pos_ratio_VBG', \n",
    "                           'pos_ratio_VBN','pos_ratio_VBZ','pos_ratio_VBD','pos_ratio_CC','pos_ratio_PRP','pos_ratio_JJ','pos_ratio_TO', \n",
    "                           'pos_ratio_IN','pos_ratio_RB']\n",
    "df = pd.concat([df,pd.json_normalize(df['pos_ratios'])[pos_tags_ratio_features].fillna(0)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linguistic_columns2 = list(set(linguistic_columns)-set(['pos_counts','pos_ratios']))\n",
    "df[linguistic_columns2] = df[linguistic_columns2].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Values Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "socure_cols = ['socure_phonerisk_reason_code','socure_addressrisk_reason_code','socure_emailrisk_reason_code'\n",
    "              ,'socure_reason_code', 'socure_kyc_reason_code']\n",
    "df_socure = df[['application_id'] + socure_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the list of reason codes\n",
    "def clean_socure_strings(x):\n",
    "    if x != None:\n",
    "        x = [val.strip(\"\"\"'|[| |\"|,|]\"\"\") for val in x.split('\\n') if not val.strip(\"\"\"'|[| |\"|,|]\"\"\") in ['',None]]\n",
    "    return x\n",
    "\n",
    "socure_cols = ['socure_phonerisk_reason_code','socure_addressrisk_reason_code','socure_emailrisk_reason_code','socure_reason_code','socure_kyc_reason_code']\n",
    "df_socure = df[['application_id'] + socure_cols]\n",
    "\n",
    "# Collecting the cleaned reason codes\n",
    "for col in socure_cols:\n",
    "    df_socure[col] = df_socure[col].str.lower()\n",
    "    df_socure[col] = df_socure[col].apply(clean_socure_strings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "socure_cols = ['socure_phonerisk_reason_code','socure_addressrisk_reason_code','socure_emailrisk_reason_code','socure_reason_code','socure_kyc_reason_code']\n",
    "\n",
    "dict_cols = {}\n",
    "for col in socure_cols:\n",
    "    df_socure = df_socure.drop(col, 1).join(df_socure[col].str.join('|').str.get_dummies())\n",
    "    socure_cols = df_socure.columns[df_socure.columns.str.startswith('socure')].to_list()\n",
    "    true_cols = list(set(df_socure.columns.to_list()) - set(socure_cols) - set(['application_id','target']))\n",
    "    new_cols = []\n",
    "    for col2 in true_cols:\n",
    "        new_cols.append(col+'_'+col2)\n",
    "    dict_cols = dict(zip(true_cols, new_cols)) | dict_cols\n",
    "    df_socure.rename(columns=dict_cols, inplace=True)\n",
    "\n",
    "df_socure = df_socure.T\n",
    "df_socure = df_socure[~df_socure.index.duplicated(keep='first')].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "socure_phone_cols = df_socure.columns[df_socure.columns.str.startswith('socure_phone')].to_list()\n",
    "socure_address_cols = df_socure.columns[df_socure.columns.str.startswith('socure_address')].to_list()\n",
    "socure_email_cols = df_socure.columns[df_socure.columns.str.startswith('socure_email')].to_list()\n",
    "socure_reason_cols = df_socure.columns[df_socure.columns.str.startswith('socure_reason')].to_list()\n",
    "socure_kyc_cols = df_socure.columns[df_socure.columns.str.startswith('socure_kyc')].to_list()\n",
    "\n",
    "df_socure['socure_phonerisk_code_count'] = df_socure[socure_phone_cols].sum(axis=1)\n",
    "df_socure['socure_addressrisk_code_count'] = df_socure[socure_address_cols].sum(axis=1)\n",
    "df_socure['socure_emailrisk_code_count'] = df_socure[socure_email_cols].sum(axis=1)\n",
    "df_socure['socure_reason_code_count'] = df_socure[socure_reason_cols].sum(axis=1)\n",
    "df_socure['socure_kyc_code_count'] = df_socure[socure_kyc_cols].sum(axis=1)\n",
    "df_socure['socure_all_codes_count'] = df_socure[socure_phone_cols+socure_address_cols+socure_email_cols+socure_reason_cols+socure_kyc_cols].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading only the trained columns and adding to the test data\n",
    "socure_cols_derived = pd.read_pickle(project_path+'models/socure_reason_codes_columns_'+year+'.pkl')\n",
    "df_tmp = pd.DataFrame(index=range(df.shape[0]),columns=socure_cols_derived['feature'].to_list())\n",
    "df_tmp = df_tmp.fillna(0)\n",
    "\n",
    "df_tmp.update(df_socure)\n",
    "df_tmp = df_tmp.astype('int')\n",
    "df = pd.concat([df,df_tmp], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "socure_kyc_field_validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def socure_kyc_field_extraction(val):\n",
    "    if val==None or val==[] or val=={} or val=='':\n",
    "        return (None, None, None, None, None, None, None, None, None)\n",
    "    else:\n",
    "        val = json.loads(val.lower())\n",
    "        city = val['city']\n",
    "        dob = val['dob']\n",
    "        firstname = val['firstname']\n",
    "        mobilenumber = val['mobilenumber']\n",
    "        ssn = val['ssn']\n",
    "        state = val['state']\n",
    "        streetaddress = val['streetaddress']\n",
    "        surname = val['surname']\n",
    "        zip_val = val['zip']\n",
    "\n",
    "        return (surname, firstname, dob, mobilenumber, ssn, state, city, streetaddress, zip_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "socure_kyc_fields_cols = ['socure_kyc_surname_score', 'socure_kyc_firstname_score', 'socure_kyc_dob_score', 'socure_kyc_mobilenumber_score', \n",
    "                         'socure_kyc_ssn_score', 'socure_kyc_state_score', 'socure_kyc_city_score', 'socure_kyc_streetaddress_score', 'socure_kyc_zip_score']\n",
    "df[socure_kyc_fields_cols] = pd.DataFrame(df.socure_kyc_field_validations.apply(socure_kyc_field_extraction).tolist(),index=df.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "purpose_of_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'purpose_of_account'\n",
    "\n",
    "# There are 5 unique purpose of accounts categories\n",
    "unique_vals = ['payroll','accounting','operating','travel expenses','business expenses']\n",
    "# Creating dummies for each unique string\n",
    "purpose_of_accounts_derived_cols = []\n",
    "for val in unique_vals:\n",
    "    new_col = 'purpose_'+val.replace(\" \", \"_\")\n",
    "    purpose_of_accounts_derived_cols.append(new_col)\n",
    "    df[new_col] = 0\n",
    "    idx = df.index[df[col].str.contains(val)==True].tolist()\n",
    "    df.loc[idx, new_col] = 1\n",
    "\n",
    "df['purpose_of_account_count'] = df[purpose_of_accounts_derived_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_cols = ['payroll','accounting','operating','travel_expenses','business_expenses']\n",
    "# for i in unique_cols:\n",
    "#     print(df.groupby(['purpose_'+i])['target'].agg(['sum', 'mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "person_fraud_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'person_fraud_tags'\n",
    "df[col] = df[col].str.lower()\n",
    "df[col+'_flag'] = np.where(df[col].str.contains('not')==True, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "person_kyc_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'person_kyc_tags'\n",
    "df[col] = df[col].str.lower()\n",
    "df[col+'_flag'] = np.where(df[col].str.contains('not')==True, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "owner_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cleaning the list of string and extracting owner name\n",
    "def clean_owner_list_strings(x):\n",
    "    if x == None:\n",
    "        return ''\n",
    "    x = x.strip(\"\"\"'|[| |\"|,|]\"\"\")\n",
    "    if x != '' :\n",
    "        try:\n",
    "            x = [val.strip(\"\"\"'|[| |\"|,|]\"\"\") for val in x.split('\\n') if not val.strip(\"\"\"'|[| |\"|,|]\"\"\") in ['',None,[]]]\n",
    "            x = ''.join(x[0].split(',')[:-3])\n",
    "            x = re.sub(r'\\s+', ' ', x)\n",
    "        except:\n",
    "            return x\n",
    "    return x\n",
    "\n",
    "df['owner_name'] = df[col].apply(clean_owner_list_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a fuzzy match score of name against email\n",
    "def name_matching_with_email(email_id: str, owner_name: str):\n",
    "    if email_id == None or email_id == '' or owner_name.strip('[|]') == '':\n",
    "        return (0,0,0)\n",
    "    # print(email_id)\n",
    "    email_id = email_id.lower()\n",
    "    owner_name = owner_name.lower()\n",
    "    \n",
    "    email_str1 = re.sub('[0-9]', '',' '.join(email_id.split('@')[0].split('.')))\n",
    "    if len(email_str1)>=3 :\n",
    "        name_str1 = [val.strip(' ') for val in re.split(' |-',owner_name) if not val.strip(' ') in ['']]\n",
    "        permute_list = list(permutations(name_str1))\n",
    "        max_ratio = 0\n",
    "        max_partial = 0\n",
    "        max_token = 0\n",
    "        name_match_score = 0\n",
    "        \n",
    "        for i in range(len(name_str1)):\n",
    "    \n",
    "            if len(name_str1[i])>=3 and name_str1[i] in email_id:\n",
    "                name_match_score = name_match_score + (100/len(name_str1))\n",
    "            \n",
    "            permute_str1 = ' '.join(list(permute_list[i]))\n",
    "            permute_str2 = ''.join(list(permute_list[i]))\n",
    "            \n",
    "            score_ratio1 = fuzz.ratio(email_str1, permute_str1)\n",
    "            score_partial1 = fuzz.partial_ratio(email_str1, permute_str1)\n",
    "            score_token1 = fuzz.token_sort_ratio(email_str1, permute_str1)\n",
    "    \n",
    "            score_ratio2 = fuzz.ratio(email_str1, permute_str2)\n",
    "            score_partial2 = fuzz.partial_ratio(email_str1, permute_str2)\n",
    "            score_token2 = fuzz.token_sort_ratio(email_str1, permute_str2)\n",
    "    \n",
    "            max_ratio = max([max_ratio, score_ratio1, score_ratio2])\n",
    "            max_partial = max([max_partial, score_partial1, score_partial2])\n",
    "            max_token = max([max_token, score_token1, score_token2])\n",
    "\n",
    "    else:\n",
    "        return (0,0,0)\n",
    "        \n",
    "    return (max_ratio, max_partial, name_match_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ownername_email_match = ['ownername_email_fuzzy_match_ratio','ownername_email_fuzzy_match', 'ownername_email_substring_match']\n",
    "df[ownername_email_match] = pd.DataFrame(df.apply(lambda x: name_matching_with_email(x['email'], x['owner_name']), axis=1\n",
    "                                                     ).tolist(),index=df.index)\n",
    "\n",
    "companyname_email_match = ['companyname_email_fuzzy_match_ratio','companyname_email_fuzzy_match', 'companyname_email_substring_match']\n",
    "df[companyname_email_match] = pd.DataFrame(df.apply(lambda x: name_matching_with_email(x['email'], x['company_name']), axis=1\n",
    "                                                     ).tolist(),index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['company_name_email_match_flag'] = np.where(((df.companyname_email_fuzzy_match_ratio>=75)|(df.companyname_email_substring_match>=20)) |\n",
    "((df.companyname_email_fuzzy_match_ratio>=70) & (df.companyname_email_fuzzy_match>=80)), 1, 0)\n",
    "\n",
    "df['owner_name_email_match_flag'] = np.where(((df.ownername_email_fuzzy_match_ratio>=75)|(df.ownername_email_substring_match>=20)) |\n",
    "((df.ownername_email_fuzzy_match_ratio>=70) & (df.ownername_email_fuzzy_match>=80)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89792, 417)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(project_path+'data/all_apps_engg_data_'+year+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_features = pd.read_pickle(project_path+'data/train_independent_features_'+year+'.pkl')['feature'].to_list()\n",
    "df = df[independent_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_dict = df.dtypes.astype(str).to_dict()\n",
    "data_type_mapping = {'float64':np.float64,'object':object,'bool':bool,'Int64':np.int64}\n",
    "data_type_dict = {k: data_type_mapping[v] for k, v in data_type_dict.items() if v in data_type_mapping}\n",
    "\n",
    "df = df.astype(data_type_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89792 entries, 0 to 89791\n",
      "Columns: 374 entries, socure_phonerisk_reason_code_r662 to dale_chall_readability_score\n",
      "dtypes: float64(90), int64(273), object(11)\n",
      "memory usage: 256.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing reason code features as these are already encoded\n",
    "\n",
    "# Encoding the categorical features\n",
    "x_object_cols = df.select_dtypes(include=['object']).columns.to_list()\n",
    "x_object_onehot = pd.get_dummies(df[x_object_cols]) # create dummies\n",
    "x_object_onehot = x_object_onehot.astype('int')\n",
    "df = pd.concat([df.drop(columns=x_object_cols), x_object_onehot], axis=1)\n",
    "df.columns = df.columns.str.lower() # convert column names to lower case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89792, 412)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter only the final independent features used in the training data\n",
    "train_cols = pd.read_pickle(project_path+'models/train_data_columns_'+year+'.pkl')['feature'].to_list()\n",
    "df_tmp = pd.DataFrame(index=range(df.shape[0]),columns=train_cols)\n",
    "df_tmp = df_tmp.fillna(0) # creating a dummy df using the columns from train dataset\n",
    "df_tmp.update(df) # update the dummy df with test df values\n",
    "df = df_tmp.copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PCA model\n",
    "with open(path+'pca_model.pkl', 'rb') as file:\n",
    "    pca_loaded = pickle.load(file)\n",
    "df = pd.DataFrame(pca_loaded.transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = 'catboost_model_pca_'+year+'.pkl'\n",
    "path = project_path + 'models/'\n",
    "model = pickle.load(open(path+file_name, \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "/Users/zomb-ml-platform-msk/go-agent-21.2.0/pipelines/BuildMaster/catboost.git/catboost/libs/data/model_dataset_compatibility.cpp:81: At position 0 should be feature with name pca_1 (found 0).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_apps_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m[:,\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.9/site-packages/catboost/core.py:5314\u001b[0m, in \u001b[0;36mCatBoostClassifier.predict_proba\u001b[0;34m(self, X, ntree_start, ntree_end, thread_count, verbose, task_type)\u001b[0m\n\u001b[1;32m   5278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ntree_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ntree_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, thread_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   5279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5280\u001b[0m \u001b[38;5;124;03m    Predict class probability with X.\u001b[39;00m\n\u001b[1;32m   5281\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5312\u001b[0m \u001b[38;5;124;03m            with probability for every class for each object.\u001b[39;00m\n\u001b[1;32m   5313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProbability\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredict_proba\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.9/site-packages/catboost/core.py:2608\u001b[0m, in \u001b[0;36mCatBoost._predict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, parent_method_name, task_type)\u001b[0m\n\u001b[1;32m   2605\u001b[0m data, data_is_single_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_predict_input_data(data, parent_method_name, thread_count)\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_prediction_type(prediction_type)\n\u001b[0;32m-> 2608\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2609\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m data_is_single_object \u001b[38;5;28;01melse\u001b[39;00m predictions\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.9/site-packages/catboost/core.py:1832\u001b[0m, in \u001b[0;36m_CatBoostBase._base_predict\u001b[0;34m(self, pool, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_base_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, pool, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type):\n\u001b[0;32m-> 1832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m_catboost.pyx:4969\u001b[0m, in \u001b[0;36m_catboost._CatBoost._base_predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4976\u001b[0m, in \u001b[0;36m_catboost._CatBoost._base_predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCatBoostError\u001b[0m: /Users/zomb-ml-platform-msk/go-agent-21.2.0/pipelines/BuildMaster/catboost.git/catboost/libs/data/model_dataset_compatibility.cpp:81: At position 0 should be feature with name pca_1 (found 0)."
     ]
    }
   ],
   "source": [
    "all_apps_pred = model.predict_proba(df)[:,1:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['prob'] = all_apps_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08426140413399857"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw[df_raw.prob>=0.6].shape[0]/df_raw.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 6, 11, 11, 27, 45, 28935)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fa8e7a0e7c7188de72acea4ae1bc222d1770499c4c3d36ce32843ef46b20053"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
